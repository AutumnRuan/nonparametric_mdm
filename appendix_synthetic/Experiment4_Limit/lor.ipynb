{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import cvxpy as cp\n",
    "from itertools import combinations\n",
    "from itertools import permutations\n",
    "import time\n",
    "import copy\n",
    "import pandas as pd\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collection_distribution(n,collection):\n",
    "    #collection = list(combinations(range(n),size))\n",
    "    collection_distribution = np.zeros((n,len(collection)))\n",
    "\n",
    "    for i in range(len(collection)):\n",
    "        distribution_S = np.random.exponential(1, len(collection[i]))\n",
    "        assortment_dis = np.array([k/np.sum(distribution_S) for k in distribution_S])\n",
    "        for j in range(len(collection[i])):\n",
    "            collection_distribution[collection[i][j]][i]= assortment_dis[j]\n",
    "            \n",
    "    return collection_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n"
     ]
    }
   ],
   "source": [
    "n = 7\n",
    "instance_size = 100\n",
    "collection_size = [2,3,5,10,15]\n",
    "#collection_size = [2,3,5]\n",
    "alpha = 1\n",
    "\n",
    "''' instance_size = 1\n",
    "collection_size = [2] '''\n",
    "\n",
    "all_size2_ass = [list(x) for x in list(combinations(range(n),2))]\n",
    "all_size3_ass = [list(x) for x in list(combinations(range(n),3))]\n",
    "#all_size4_ass = [list(x) for x in list(combinations(range(n),4))]\n",
    "#all_possible_assortment = all_size2_ass + all_size3_ass + all_size4_ass\n",
    "all_possible_assortment = all_size2_ass + all_size3_ass \n",
    "print(len(all_possible_assortment))\n",
    "\n",
    "std = 0.01\n",
    "\n",
    "all_collection = [[] for _ in range(len(collection_size))]\n",
    "\n",
    "# construct 1000 samples of random assortment size collection of first collection size\n",
    "for j in range(instance_size):\n",
    "    collection = []\n",
    "    while(len(collection)<collection_size[0]):\n",
    "        S = all_possible_assortment[np.random.randint(len(all_possible_assortment))]\n",
    "        if S not in collection:\n",
    "            collection.append(S)\n",
    "    all_collection[0].append(collection)\n",
    "\n",
    "\n",
    "for i in range(1,len(collection_size)):\n",
    "    all_collection[i] = copy.deepcopy(all_collection[i-1])\n",
    "    for j in range(instance_size):\n",
    "        while(len(all_collection[i][j])<collection_size[i]):\n",
    "            S = all_possible_assortment[np.random.randint(len(all_possible_assortment))]\n",
    "            if S not in all_collection[i][j]:\n",
    "                all_collection[i][j].append(S)\n",
    "\n",
    "all_instance = []\n",
    "for i in range(len(collection_size)):\n",
    "    collection_ins = []   \n",
    "    for j in range(instance_size):\n",
    "        ''' v = np.random.normal(0,1,size=n) #standard normal\n",
    "        curr_coll_mnl = collection_distribution_mnl(n,all_collection[i][j],v)\n",
    "        collection_ins.append(gaussian_noise(curr_coll_mnl,std,alpha)) '''\n",
    "        collection_ins.append(collection_distribution(n,all_collection[i][j]))\n",
    "    all_instance.append(collection_ins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rum_limit(data):\n",
    "\n",
    "    # data processing for RUM feasibility checking\n",
    "    n,m = data.shape\n",
    "    asmnt = []\n",
    "    for i in range(m):\n",
    "        curr_assortment = np.where(data[:,i]!=0)\n",
    "        for j in curr_assortment:\n",
    "            asmnt.append(list(j))\n",
    "   \n",
    "    # flatten the data to dimension 1\n",
    "    data_zip = data.flatten()\n",
    "    for j in np.where(data_zip!=0):\n",
    "        index_in = j\n",
    "    index_in.astype(int)\n",
    "    index_in= list(index_in)\n",
    "\n",
    "    #specify all the possible permutation lists for each element in the probability collection\n",
    "    perm_tab = [[] for _ in range(n*m)]\n",
    "    perm = list(permutations(range(n),n))\n",
    "    perm = [list(i) for i in perm]\n",
    "\n",
    "    for i in range(m):\n",
    "        for j in asmnt[i]:\n",
    "            for p in range(len(perm)):\n",
    "            #for p in perm:\n",
    "                new_p = [k for k in perm[p] if k in asmnt[i]]\n",
    "                if j == new_p[0]:\n",
    "                    perm_tab[i+j*m].append(p)\n",
    "    \n",
    "    lam = cp.Variable(shape=len(perm),nonneg = True)\n",
    "    x = cp.Variable(shape = n*m ,nonneg = True)\n",
    "    \n",
    "    constraints = [sum(x[j] for j in asmnt[i])==1 for i in range(m)] # normalization conditions\n",
    "    \n",
    "    constraints = [sum(lam[i] for i in range(len(perm)))==1]\n",
    "    for i in index_in:\n",
    "        constraints+=[sum(lam[j] for j in perm_tab[i])- x[i] == 0]\n",
    "    \n",
    "    #obj = cp.Minimize(cp.norm1(x-data_zip))\n",
    "    obj = cp.Minimize(sum(data_zip[k]*cp.abs(data_zip[k] - x[k]) for k in range(n*m)))\n",
    "    \n",
    "    prob = cp.Problem(obj,constraints)\n",
    "    prob.solve(solver =cp.GUROBI)\n",
    "    \n",
    "    if prob.value < math.pow(10,-6):\n",
    "        return [0,prob.solver_stats.solve_time] \n",
    "    else:\n",
    "        return [prob.value,prob.solver_stats.solve_time] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting testing the limit of rum\n",
      "Academic license - for non-commercial use only - expires 2023-07-14\n",
      "Using license file /Users/autumn/gurobi.lic\n",
      "the total running time 6691.868456125259\n"
     ]
    }
   ],
   "source": [
    "print('starting testing the limit of rum')\n",
    "start_tinme = time.time()\n",
    "rum_runtime = []\n",
    "rum_total_loss = []  #of different collection size \n",
    "rum_mean_loss = []\n",
    "rum_loss_se = []\n",
    "rum_runtime_se = []\n",
    "for i in range(len(all_instance)):\n",
    "    loss = []\n",
    "    runtime = [] # of the same collection size\n",
    "    for j in range(len(all_instance[i])):\n",
    "        result = rum_limit(all_instance[i][j])\n",
    "        loss.append(result[0])\n",
    "        runtime.append(result[1])\n",
    "        \n",
    "    test = pd.DataFrame({'loss':loss,'runtime':runtime})\n",
    "    test.to_csv('rum/collection_'+str(collection_size[i])+'.csv')\n",
    "    runtime = np.array(runtime)\n",
    "    rum_runtime.append(np.mean(runtime))\n",
    "    rum_runtime_se.append(np.std(runtime)/np.sqrt(len(runtime)))\n",
    "    loss = np.array(loss)\n",
    "    rum_mean_loss.append(np.mean(loss))\n",
    "    rum_total_loss.append(np.sum(loss))\n",
    "    rum_loss_se.append(np.std(loss)/np.sqrt(len(loss)))\n",
    "    \n",
    "print('the total running time',time.time()-start_tinme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rum = pd.DataFrame({'collection_size':collection_size,'rum_mean_oss':rum_mean_loss,'rum_loss_se':rum_loss_se,'rum_mean_runtime':rum_runtime,'rum_runtime_se':rum_runtime_se})\n",
    "rum.to_csv('rum_limit_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.009100324947453378,\n",
       " 0.04751833290086248,\n",
       " 0.1370108106694161,\n",
       " 0.56030484511451,\n",
       " 1.1690020351278674]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rum_mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0.0003337860107421875]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = np.array([[0.3,0.9,0],[0.7,0,0.8],[0,0.1,0.2]])\n",
    "result1 = rum_limit(data1)\n",
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0.0002968311309814453]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = np.array([[7/20,2/8],[2/8,7/20],[2/5,0],[0,2/5]])\n",
    "result2 = rum_limit(data2)\n",
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.015000000000000038, 0.0004999637603759766]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3 = np.array([[0.1,0.2,0.2,0.25],[0.2,0.25,0.25,0.75],[0.2,0.55,0,0],[0.5,0,0.55,0]])\n",
    "result3 = rum_limit(data3)\n",
    "result3"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ebdf0563cd0506869acfe44a7cbfc0f9ee11e0c875e1179c386aef281c5ba524"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('anaconda3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
