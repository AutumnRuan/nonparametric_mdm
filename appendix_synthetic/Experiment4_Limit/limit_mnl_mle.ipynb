{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import cvxpy as cp\n",
    "from itertools import combinations\n",
    "import time\n",
    "import copy\n",
    "import pandas as pd\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collection_distribution(n,collection):\n",
    "    #collection = list(combinations(range(n),size))\n",
    "    collection_distribution = np.zeros((n,len(collection)))\n",
    "\n",
    "    for i in range(len(collection)):\n",
    "        distribution_S = np.random.exponential(1, len(collection[i]))\n",
    "        assortment_dis = np.array([k/np.sum(distribution_S) for k in distribution_S])\n",
    "        for j in range(len(collection[i])):\n",
    "            collection_distribution[collection[i][j]][i]= assortment_dis[j]\n",
    "            \n",
    "    return collection_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 7\n",
    "instance_size = 100\n",
    "#collection_size = [2,3,5,10,15,20]\n",
    "collection_size = [2,3,5,10,15]\n",
    "#collection_size = [2,3,5]\n",
    "\n",
    "all_size2_ass = [list(x) for x in list(combinations(range(n),2))]\n",
    "all_size3_ass = [list(x) for x in list(combinations(range(n),3))]\n",
    "#all_size4_ass = [list(x) for x in list(combinations(range(n),4))]\n",
    "#all_possible_assortment = all_size2_ass + all_size3_ass + all_size4_ass\n",
    "all_possible_assortment = all_size2_ass + all_size3_ass \n",
    "\n",
    "all_collection = [[] for _ in range(len(collection_size))]\n",
    "\n",
    "# construct 1000 samples of random assortment size collection of first collection size\n",
    "for j in range(instance_size):\n",
    "    collection = []\n",
    "    while(len(collection)<collection_size[0]):\n",
    "        S = all_possible_assortment[np.random.randint(len(all_possible_assortment))]\n",
    "        if S not in collection:\n",
    "            collection.append(S)\n",
    "    all_collection[0].append(collection)\n",
    "\n",
    "\n",
    "for i in range(1,len(collection_size)):\n",
    "    all_collection[i] = copy.deepcopy(all_collection[i-1])\n",
    "    for j in range(instance_size):\n",
    "        while(len(all_collection[i][j])<collection_size[i]):\n",
    "            S = all_possible_assortment[np.random.randint(len(all_possible_assortment))]\n",
    "            if S not in all_collection[i][j]:\n",
    "                all_collection[i][j].append(S)\n",
    "\n",
    "all_instance = []\n",
    "for i in range(len(collection_size)):\n",
    "    collection_ins = []   \n",
    "    for j in range(instance_size):\n",
    "        collection_ins.append(collection_distribution(n,all_collection[i][j]))\n",
    "        ''' v = np.random.normal(0,1,size=n) #standard normal\n",
    "        curr_coll_mnl = collection_distribution_mnl(n,all_collection[i][j],v)\n",
    "        collection_ins.append(gaussian_noise(curr_coll_mnl,std,alpha)) '''\n",
    "    all_instance.append(collection_ins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = all_instance[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(data):\n",
    "    not_row = data[[not np.all(data[i] == 0) for i in range(data.shape[0])], :]  ### only keep the rows where the data points are not all zero meaning that the product is included in the assortments\n",
    "    \n",
    "    pre_collection = copy.deepcopy(not_row.T)\n",
    "    collection = []\n",
    "    for i in range(len(pre_collection)):\n",
    "        collection.append(list(np.nonzero(pre_collection[i])[0]))\n",
    "\n",
    "    binary_pre_collection = copy.deepcopy(not_row.T)\n",
    "    binary_pre_collection[binary_pre_collection >0] =1\n",
    "    binary_pre_collection[binary_pre_collection ==0] =-5000000\n",
    "    binary_pre_collection = binary_pre_collection.astype(int)\n",
    "    binary_collection = binary_pre_collection.tolist()\n",
    "    \n",
    "    return not_row,collection,binary_collection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.81484605, 0.72923242],\n",
       "        [0.11280148, 0.        ],\n",
       "        [0.07235248, 0.09655199],\n",
       "        [0.        , 0.17421559]]),\n",
       " [[0, 1, 2], [0, 2, 3]],\n",
       " [[1, 1, 1, -5000000], [1, -5000000, 1, 1]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_process(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2)\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "result = data_process(data)\n",
    "print(result[0].shape)\n",
    "print(len(result[1]))\n",
    "print(len(result[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvxpy import log_sum_exp\n",
    "\n",
    "def mnl_mle(data,curr_collection,collection):\n",
    "    #print(data)\n",
    "    n = data.shape[0]\n",
    "    v= cp.Variable(n)\n",
    "    ''' test_v = list(range(n)) \n",
    "    test_v = [x+1 for x in test_v]\n",
    "    print('testing',test_v) '''\n",
    "    #print(len(curr_collection))\n",
    "    #collection = [np.array([1,1,1,0]),np.array([1,0,1,1]),np.array([1,0,0,1])]\n",
    "    obj = 0\n",
    "    for i in range(len(curr_collection)): \n",
    "        for j in curr_collection[i]:\n",
    "            #print(cp.log_sum_exp(np.array(collection[i]).T@test_v))\n",
    "            #print(cp.log(np.array(collection[i])@ cp.exp(test_v)))\n",
    "            #print(cp.log(cp.sum(cp.multiply(np.array(collection[i]),cp.exp(test_v)),keepdims=True)))\n",
    "            #obj = obj + data[j][i]* (v[j] - cp.log(cp.sum(cp.multiply(np.array(collection[i]),cp.exp(v)),keepdims=True)))\n",
    "            #print('testing',cp.multiply(collection[i],test_v))\n",
    "            #print('check collection',collection[i])\n",
    "            obj = obj + data[j][i]*v[j]\n",
    "        obj = obj - cp.log_sum_exp(cp.multiply(collection[i],v))\n",
    "            #obj = obj + data[j][i]* (v[j] - cp.log_sum_exp(cp.multiply(collection[i],v)))\n",
    "         \n",
    "    prob = cp.Problem(cp.Maximize(obj))\n",
    "    prob.solve()\n",
    "    #print(v.value)\n",
    "    return v.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' test_data = np.array([[1/2,1/2,0,0,0],[0,0,1/3,1/3,1/3]]).T\\ntest_data.shape '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test_data = np.array([[1/3,1/3,1/3,0],[1/3,0,1/3,1/3]]).T\n",
    "\n",
    "''' test_data = np.array([[1/2,1/2,0,0,0],[0,0,1/3,1/3,1/3]]).T\n",
    "test_data.shape '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mnl_mle(test_data, [[0,1],[2,3,4]], [[1,1,0,0,0],[0,0,1,1,1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' print(pro_data)\\nfor i in range(len(collection)): \\n    for j in collection[i]:\\n        print(pro_data[j][i]) '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pro_data, collection, b_collection = data_process(test_data)\n",
    "''' print(pro_data)\n",
    "for i in range(len(collection)): \n",
    "    for j in collection[i]:\n",
    "        print(pro_data[j][i]) '''\n",
    "        #obj = obj + data[j][i]* (v[j] - cp.log_sum_exp(v))\n",
    "\n",
    "#mnl_mle(pro_data, collection, b_collection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnl_ditribution(v,collection):\n",
    "    collection_distribution = np.zeros((len(v),len(collection)))\n",
    "    for i in range(len(collection)):\n",
    "        curr_total_v = 0\n",
    "        for j in collection[i]:\n",
    "            curr_total_v = curr_total_v + math.exp(v[j])\n",
    "        for k in range(len(collection[i])):\n",
    "            collection_distribution[collection[i][k]][i] = math.exp(v[collection[i][k]])/curr_total_v \n",
    "    return collection_distribution  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(mnl_data,sample_data):\n",
    "    loss = 0\n",
    "    for i in range(sample_data.shape[0]):\n",
    "        for j in range(sample_data.shape[1]):\n",
    "            #loss = loss + np.abs(sample_data[i][j]-mnl_data[i][j])\n",
    "            loss = loss + sample_data[i][j] * np.abs(sample_data[i][j]-mnl_data[i][j])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting testing the limit of MNL\n",
      "the total running time 24.63560700416565\n"
     ]
    }
   ],
   "source": [
    "print('starting testing the limit of MNL')\n",
    "start_time = time.time()\n",
    "mnl_runtime = []\n",
    "mnl_total_loss = []  #of different collection size \n",
    "mnl_mean_loss = []\n",
    "mnl_loss_se = []\n",
    "mnl_runtime_se = []\n",
    "for i in range(len(all_instance)):\n",
    "    loss = []\n",
    "    runtime = [] # of the same collection size\n",
    "    for j in range(len(all_instance[i])):\n",
    "        start_run_time = time.time()\n",
    "        curr_instance,collection,binary_collection = data_process(all_instance[i][j])\n",
    "        v = mnl_mle(curr_instance,collection,binary_collection)\n",
    "        if len(v)==0:\n",
    "            continue\n",
    "        estimated_mnl_data = mnl_ditribution(v,collection)\n",
    "        curr_loss = compute_loss(estimated_mnl_data,curr_instance)\n",
    "        used_time = time.time() - start_run_time\n",
    "        loss.append(curr_loss)\n",
    "        runtime.append(used_time)\n",
    "        \n",
    "    test = pd.DataFrame({'loss':loss,'runtime':runtime})\n",
    "    test.to_csv('mnl/collection_'+str(collection_size[i])+'.csv')\n",
    "    runtime = np.array(runtime)\n",
    "    mnl_runtime.append(np.mean(runtime))\n",
    "    mnl_runtime_se.append(np.std(runtime)/np.sqrt(len(runtime)))\n",
    "    loss = np.array(loss)\n",
    "    mnl_mean_loss.append(np.mean(loss))\n",
    "    mnl_total_loss.append(np.sum(loss))\n",
    "    mnl_loss_se.append(np.std(loss)/np.sqrt(len(loss)))\n",
    "    \n",
    "print('the total running time',time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnl = pd.DataFrame({'collection_size':collection_size,'mnl_mean_oss':mnl_mean_loss,'mnl_loss_se':mnl_loss_se,'mnl_mean_runtime':mnl_runtime,'mnl_runtime_se':mnl_runtime_se})\n",
    "mnl.to_csv('mnl_mle_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.025551576765766824,\n",
       " 0.13534370594345396,\n",
       " 0.4990971863587697,\n",
       " 1.6802910061161078,\n",
       " 2.8867146205778997]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnl_mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.008150273132167957,\n",
       " 0.019855244328794885,\n",
       " 0.028390855764469397,\n",
       " 0.03711771081375096,\n",
       " 0.05771037135164159]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnl_loss_se"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('anaconda3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ebdf0563cd0506869acfe44a7cbfc0f9ee11e0c875e1179c386aef281c5ba524"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
